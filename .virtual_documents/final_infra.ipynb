!pip install mlflow


import mlflow
from mlflow.models import infer_signature

import pandas as pd
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
mlflow.set_tracking_uri("http://127.0.0.1:5000")



# import requests

# # API endpoint URL
# url = "http://localhost/api/database/204a70a0-3dc2-4b8a-b36f-7c266d6d750e/table/183c11ba-3e3b-4d39-835d-f33d03393802/data"

# # Define the headers
# headers = {
#     "Accept": "application/json"  # Specify the expected response format
# }

# try:
#     # Send a GET request to the API with the Accept header
#     response = requests.get(url, headers=headers)

#     # Check if the request was successful
#     if response.status_code == 200:
#         # Parse the JSON response
#         data = response.json()
#         print("API Response:", data)
#         print( data.count)
#     else:
#         print(f"Error: Received status code {response.status_code}")
#         print("Response content:", response.text)
       

# except requests.exceptions.RequestException as e:
#     print(f"Request failed: {e}")



# import requests

# # API endpoint URL
# base_url = "http://localhost/api/database/204a70a0-3dc2-4b8a-b36f-7c266d6d750e/table/183c11ba-3e3b-4d39-835d-f33d03393802/data"

# # Define the headers
# headers = {
#     "Accept": "application/json"  # Specify the expected response format
# }

# # Initialize pagination variables
# page = 1
# limit = 10  # Number of records per request
# all_data = []

# # Set target number of records to fetch
# target_records = 20

# # Fetch data with pagination until we reach the target records
# while len(all_data) < target_records:
#     # Construct the URL with the current page and limit
#     url = f"{base_url}?limit={limit}&offset={limit * (page - 1)}"
#     print(f"Requesting page {page}... (URL: {url})")
    
#     try:
#         # Send a GET request to the API with the Accept header
#         response = requests.get(url, headers=headers)

#         # Check if the request was successful
#         if response.status_code == 200:
#             # Parse the JSON response
#             data = response.json()

#             # Debugging: Print response data length and check if it's empty
#             print(f"Received {len(data)} records on page {page}.")
            
#             if not data:
#                 print("No more data to fetch. Breaking the loop.")
#                 break

#             # Add the fetched data to the all_data list
#             all_data.extend(data)

#             # Increment the page number for the next request
#             page += 1

#             # If we've reached the target number of records, stop
#             if len(all_data) >= target_records:
#                 break

#         else:
#             print(f"Error: Received status code {response.status_code}")
#             print("Response content:", response.text)
#             break

#     except requests.exceptions.RequestException as e:
#         print(f"Request failed: {e}")
#         break

# # After the loop, all_data will contain all records fetched
# print(f"Total records fetched: {len(all_data)}")



# Load the Iris dataset
# df = pd.DataFrame(data)

# # Convert the columns with measurements from strings to float type
# df['sepallengthcm'] = df['sepallengthcm'].astype(float)
# df['sepalwidthcm'] = df['sepalwidthcm'].astype(float)
# df['petallengthcm'] = df['petallengthcm'].astype(float)
# df['petalwidthcm'] = df['petalwidthcm'].astype(float)

# # Display the dataframe to verify
# print(df.count())


# # Features (X) - all columns except 'species'
# X = df[['sepallengthcm', 'sepalwidthcm', 'petallengthcm', 'petalwidthcm']]

# # Labels (y) - the 'species' column
# y = df['species']



# print(y.value_counts())




X, y = datasets.load_iris(return_X_y=True)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
# Define the model hyperparameters
params = {
    "solver": "lbfgs",
    "max_iter": 1000,
    "multi_class": "auto",
    "random_state": 8888,
}

# Train the model
lr = LogisticRegression(**params)
lr.fit(X_train, y_train)

# Predict on the test set
y_pred = lr.predict(X_test)

# Calculate metrics
accuracy = accuracy_score(y_test, y_pred)



# Set our tracking server uri for logging
mlflow.set_tracking_uri(uri="http://127.0.0.1:5000")

# Create a new MLflow Experiment
mlflow.set_experiment("MLflow Quickstart")

# Start an MLflow run
with mlflow.start_run():
    # Log the hyperparameters
    mlflow.log_params(params)

    # Log the loss metric
    mlflow.log_metric("accuracy", accuracy)

    # Set a tag that we can use to remind ourselves what this run was for
    mlflow.set_tag("Training Info", "Basic LR model for iris data")

    # Infer the model signature
    signature = infer_signature(X_train, lr.predict(X_train))

    # Log the model
    model_info = mlflow.sklearn.log_model(
        sk_model=lr,
        artifact_path="iris_model",
        signature=signature,
        input_example=X_train,
        registered_model_name="tracking-quickstart",
    )


# Load the model back for predictions as a generic Python Function model
loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)

predictions = loaded_model.predict(X_test)

iris_feature_names = datasets.load_iris().feature_names

result = pd.DataFrame(X_test, columns=iris_feature_names)
result["actual_class"] = y_test
result["predicted_class"] = predictions

result[:4]


import joblib

# Save the trained model to a .pkl file
joblib.dump(model, 'iris_classifier.pkl')

print("Model saved as 'iris_classifier.pkl'")



import requests
import json

# Define the URL and token
url = "https://127.0.0.1:5000/api/records"
token = "FGIxr9BpYjRNO5MFhxcRAnLeIqvlLmXxtoA9qNcrLKw9lkCrIj29CplQkFzk"  

# Define the headers
headers = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {token}"
}

# Define the data to be sent in the request
data = {
    "title": "My trined ML model",
    "description": "This is a sample unstructed data of a simple trained ML model",
    "creator": "Reema Dass"
}

# Send the POST request
response = requests.post(url, headers=headers, data=json.dumps(data), verify=False)

# Print the response
print(response.status_code)
print(response.json())
# print(json.dumps(response, indent=4, ensure_ascii=False))


import requests
import json

# Define the URL and token
url = "https://127.0.0.1:5000/api/records/3m20n-wwx06/draft/files"
token = "FGIxr9BpYjRNO5MFhxcRAnLeIqvlLmXxtoA9qNcrLKw9lkCrIj29CplQkFzk"  

# Define the headers
headers = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {token}"
}

# Define the data to be sent in the request
data = [
    {"key": "ML_model"}
]

# Send the POST request
response = requests.post(url, headers=headers, data=json.dumps(data), verify=False)

# Print the response
print(response.status_code)
print(response.json())



import requests

# Define the URL and token
url = "https://127.0.0.1:5000/api/records/3m20n-wwx06/draft/files/ML_model/content"
token = "FGIxr9BpYjRNO5MFhxcRAnLeIqvlLmXxtoA9qNcrLKw9lkCrIj29CplQkFzk"  

# Define the headers
headers = {
    "Content-Type": "application/octet-stream",
    "Authorization": f"Bearer {token}"
}

# Open the file you want to upload in binary mode
with open('iris_classifier.pkl', 'rb') as file:  # Replace with the actual .pkl file name
    # Send the PUT request
    response = requests.put(url, headers=headers, data=file, verify=False)

# Print the response
print(response.status_code)
print(response.json())



import requests

# Define the URL and token
url = "https://127.0.0.1:5000/api/records/3m20n-wwx06/draft/files/ML_model/commit"
token = "FGIxr9BpYjRNO5MFhxcRAnLeIqvlLmXxtoA9qNcrLKw9lkCrIj29CplQkFzk"  

# Define the headers
headers = {
    "Authorization": f"Bearer {token}"
}

# Send the POST request
response = requests.post(url, headers=headers, verify=False)

# Print the response
print(response.status_code)
print(response.json())



import requests

# Define the URL to retrieve metadata
record_id = "3m20n-wwx06"  # Replace with your actual record ID
url = f"https://127.0.0.1:5000/api/records/{record_id}"

# Your authorization token
token = "FGIxr9BpYjRNO5MFhxcRAnLeIqvlLmXxtoA9qNcrLKw9lkCrIj29CplQkFzk"

# Define the headers
headers = {
    "Authorization": f"Bearer {token}"
}

# Send the GET request to retrieve metadata
response = requests.get(url, headers=headers, verify=False)

# Check if the request was successful
if response.status_code == 200:
    metadata = response.json()
    print("Metadata Retrieved Successfully!")
    print(metadata)
else:
    print("Error:", response.status_code)
    print(response.text)



import requests

# Define the URL to retrieve metadata
record_id = "3m20n-wwx06"  
url = f"https://127.0.0.1:5000/api/records/{record_id}"

# Your authorization token
token = "FGIxr9BpYjRNO5MFhxcRAnLeIqvlLmXxtoA9qNcrLKw9lkCrIj29CplQkFzk"

# Define the headers
headers = {
    "Authorization": f"Bearer {token}"
}

# Send the GET request to retrieve metadata
response = requests.get(url, headers=headers, verify=False)

if response.status_code == 200:
    record = response.json()
    print("Detailed Metadata:")
    metadata = record.get('metadata', {})
    print(json.dumps(metadata, indent=4, ensure_ascii=False))
    print('metadata keys')
    for key in metadata.keys(): print (key)
else:
    print(f"Failed to fetch metadata. Status code: {response.status_code}")
    print(response.text)


